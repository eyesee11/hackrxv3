# Lightning-Fast Local RAG System Architecture

## Your Selected Stack Overview
**Target: <300ms average response time with high accuracy**
**Deployment: Local system + ngrok tunneling**

## 1. Vector Database & Search
### **ChromaDB + SQLite (In-Memory Mode)**
```python
import chromadb
from chromadb.config import Settings

# Ultra-fast in-memory configuration
client = chromadb.Client(Settings(
    chroma_db_impl="duckdb+parquet",
    persist_directory=None,  # In-memory mode
    anonymized_telemetry=False
))

collection = client.create_collection(
    name="documents",
    metadata={"hnsw:space": "cosine", "hnsw:M": 16}  # Optimized HNSW
)
```

**Performance Benefits:**
- Zero network latency
- RAM-speed access (~1-5ms search time)
- No external dependencies
- Perfect for hackathon demos

**âš ï¸ Recommendations:**
- **RAM Requirements**: Ensure 4-8GB free RAM for large document sets
- **Persistence Strategy**: Consider periodic saves during long sessions
- **Backup Plan**: Keep a disk-persisted version for data safety

## 2. Embeddings
### **Google's text-embedding-004 Model**
```python
import google.generativeai as genai
from typing import List

genai.configure(api_key="your-api-key")

async def get_embedding_batch(texts: List[str], batch_size: int = 100):
    """Batch embeddings for better performance"""
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        # Google's latest embedding model
        result = await genai.embed_content(
            model="models/text-embedding-004",
            content=batch,
            task_type="retrieval_document"  # Optimized for RAG
        )
        
        embeddings.extend(result['embedding'])
        
    return embeddings
```

**Performance Profile:**
- **Latency**: ~30-80ms per request
- **Dimensions**: 768 (good balance of accuracy/speed)
- **Context Length**: 2048 tokens
- **Cost**: Very affordable compared to OpenAI

**ðŸš€ My Recommendations:**
1. **Batch Processing**: Always embed multiple texts together
2. **Task Type Optimization**: Use `retrieval_document` for documents, `retrieval_query` for queries
3. **Caching Strategy**: Cache embeddings aggressively (they don't change)
4. **Rate Limiting**: Google has generous limits but implement backoff

## 3. LLM Generation
### **Primary: Gemini 2.0 Flash Lite**
```python
import google.generativeai as genai

# Configure for maximum speed
generation_config = {
    "temperature": 0.1,
    "top_p": 0.8,
    "top_k": 20,
    "max_output_tokens": 200,  # Keep responses concise for speed
    "response_mime_type": "text/plain",
}

model = genai.GenerativeModel(
    model_name="gemini-2.0-flash-lite",
    generation_config=generation_config,
)

async def generate_response(query: str, context: str):
    prompt = f"""Based on the following context, answer the question concisely:

Context: {context}

Question: {query}

Answer:"""
    
    response = await model.generate_content_async(prompt)
    return response.text
```

### **Backup: Together.ai**
```python
import together

together.api_key = "your-together-key"

def generate_with_together(query: str, context: str):
    response = together.Complete.create(
        prompt=f"Context: {context}\n\nQ: {query}\nA:",
        model="meta-llama/Llama-2-7b-chat-hf",
        max_tokens=150,
        temperature=0.1,
        stop=["Q:", "\n\n"]
    )
    return response['output']['choices'][0]['text']
```

**ðŸŽ¯ Speed Optimization Tips:**
1. **Prompt Engineering**: Keep prompts concise and structured
2. **Token Limits**: Cap responses at 150-200 tokens
3. **Streaming**: Implement streaming for perceived speed
4. **Fallback Logic**: Auto-switch to Together.ai if Gemini is slow

## 4. Backend Framework
### **FastAPI + uvloop + Optimizations**
```python
import uvloop
import asyncio
from fastapi import FastAPI, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from functools import lru_cache
import time
from contextlib import asynccontextmanager

# Install uvloop for better async performance
uvloop.install()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup: Pre-load models and warm up caches
    await warm_up_system()
    yield
    # Shutdown: Clean up resources

app = FastAPI(lifespan=lifespan)

# Optimized CORS for local development
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Restrict in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Performance monitoring middleware
@app.middleware("http")
async def add_process_time_header(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response
```

## 5. In-Memory Caching Strategy
### **Multi-Level Caching**
```python
from functools import lru_cache
import hashlib
import pickle
from typing import Dict, Any

class AdvancedCache:
    def __init__(self):
        self.embedding_cache: Dict[str, Any] = {}
        self.query_cache: Dict[str, Any] = {}
        self.context_cache: Dict[str, Any] = {}
    
    def cache_key(self, text: str) -> str:
        return hashlib.md5(text.encode()).hexdigest()
    
    @lru_cache(maxsize=1000)
    def get_cached_embedding(self, text: str):
        """LRU cache for embeddings"""
        return self._compute_embedding(text)
    
    def cache_query_result(self, query: str, result: dict):
        """Cache complete query results"""
        key = self.cache_key(query)
        self.query_cache[key] = {
            'result': result,
            'timestamp': time.time()
        }
    
    def get_cached_query(self, query: str, max_age: int = 300):
        """Get cached query if not expired"""
        key = self.cache_key(query)
        if key in self.query_cache:
            cached = self.query_cache[key]
            if time.time() - cached['timestamp'] < max_age:
                return cached['result']
        return None

# Global cache instance
cache = AdvancedCache()
```

**ðŸ“ˆ Caching Strategy:**
1. **Embeddings**: Cache indefinitely (they don't change)
2. **Query Results**: Cache for 5 minutes with LRU eviction
3. **Context Chunks**: Pre-compute and cache frequent combinations
4. **Model Responses**: Cache identical queries for instant responses

## 6. Document Processing Pipeline
### **Optimized Chunking & Processing**
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter
import tiktoken
from typing import List, Dict

class OptimizedDocumentProcessor:
    def __init__(self):
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,  # Optimal for Google's embedding model
            chunk_overlap=50,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
            length_function=self.count_tokens
        )
    
    def count_tokens(self, text: str) -> int:
        return len(self.tokenizer.encode(text))
    
    async def process_documents(self, documents: List[str]) -> List[Dict]:
        processed_chunks = []
        
        for doc_id, content in enumerate(documents):
            # Smart chunking with metadata
            chunks = self.splitter.split_text(content)
            
            for chunk_id, chunk in enumerate(chunks):
                if len(chunk.strip()) < 50:  # Skip tiny chunks
                    continue
                
                processed_chunks.append({
                    'content': chunk,
                    'metadata': {
                        'doc_id': doc_id,
                        'chunk_id': chunk_id,
                        'token_count': self.count_tokens(chunk)
                    }
                })
        
        return processed_chunks
```

## 7. Performance Optimizations
### **A. Parallel Processing Pipeline**
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import List, Tuple

async def parallel_rag_pipeline(query: str) -> dict:
    """Fully parallelized RAG pipeline"""
    start_time = time.time()
    
    # Step 1: Check cache first
    cached_result = cache.get_cached_query(query)
    if cached_result:
        return cached_result
    
    # Step 2: Parallel embedding and preprocessing
    loop = asyncio.get_event_loop()
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        tasks = [
            loop.run_in_executor(executor, get_embedding_cached, query),
            loop.run_in_executor(executor, preprocess_query, query),
        ]
        
        query_embedding, processed_query = await asyncio.gather(*tasks)
    
    # Step 3: Vector search
    search_results = await search_vectors_fast(query_embedding, top_k=5)
    
    # Step 4: Context preparation and generation in parallel
    context_task = loop.run_in_executor(executor, prepare_context, search_results)
    context = await context_task
    
    # Step 5: Generate response
    response = await generate_response(processed_query, context)
    
    # Step 6: Cache result
    result = {
        'response': response,
        'response_time_ms': (time.time() - start_time) * 1000,
        'sources': [r.metadata for r in search_results]
    }
    
    cache.cache_query_result(query, result)
    return result
```

### **B. System Warm-up**
```python
async def warm_up_system():
    """Pre-load everything for faster first request"""
    # Warm up embedding model
    await get_embedding_batch(["test query for warmup"])
    
    # Warm up LLM
    await generate_response("test", "test context")
    
    # Pre-compute common embeddings
    common_queries = ["what is", "how to", "explain", "summarize"]
    for query in common_queries:
        cache.get_cached_embedding(query)
    
    print("âœ… System warmed up and ready!")
```

## 8. Ngrok Setup & Configuration
### **Optimized Ngrok Configuration**
```bash
# Install ngrok
npm install -g ngrok

# Create ngrok.yml for better performance
version: "2"
authtoken: your-auth-token
tunnels:
  rag-api:
    addr: 8000
    proto: http
    bind_tls: true
    inspect: false  # Disable for better performance
    region: us      # Choose closest region
```

```python
# In your FastAPI app
if __name__ == "__main__":
    import uvicorn
    
    # Production-like settings for local development
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        workers=1,  # Single worker for local development
        loop="uvloop",
        access_log=False,  # Disable for better performance
        reload=False
    )
```

## 9. Expected Performance Profile

| Component | Expected Time | Your Optimization |
|-----------|---------------|-------------------|
| In-memory vector search | 1-5ms | ChromaDB RAM-only |
| Google embedding | 30-80ms | Batch processing + cache |
| Gemini 2.0 Flash generation | 100-200ms | Concise prompts + streaming |
| Framework overhead | 5-10ms | FastAPI + uvloop |
| Cache hits | 1-2ms | Multi-level in-memory |
| **Total (cache miss)** | **136-295ms** | **âœ… Under target** |
| **Total (cache hit)** | **6-17ms** | **ðŸš€ Lightning fast** |

## 10. My Additional Recommendations

### **ðŸ”§ System Requirements**
- **RAM**: Minimum 8GB, recommended 16GB
- **CPU**: Multi-core preferred for parallel processing
- **Storage**: SSD for faster model loading
- **Network**: Stable internet for API calls

### **âš¡ Performance Boosters**
1. **Pre-process Everything**: Embed all documents at startup
2. **Aggressive Caching**: Cache at every possible level
3. **Connection Pooling**: Reuse HTTP connections for APIs
4. **Response Streaming**: Start sending response while generating
5. **Batch Operations**: Group API calls when possible

### **ðŸ›¡ï¸ Reliability Measures**
```python
# Circuit breaker pattern for API reliability
import asyncio
from datetime import datetime, timedelta

class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "closed"  # closed, open, half-open
    
    async def call(self, func, *args, **kwargs):
        if self.state == "open":
            if datetime.now() - self.last_failure_time > timedelta(seconds=self.timeout):
                self.state = "half-open"
            else:
                raise Exception("Circuit breaker is open")
        
        try:
            result = await func(*args, **kwargs)
            if self.state == "half-open":
                self.state = "closed"
                self.failure_count = 0
            return result
        except Exception as e:
            self.failure_count += 1
            if self.failure_count >= self.failure_threshold:
                self.state = "open"
                self.last_failure_time = datetime.now()
            raise e
```

### **ðŸ“Š Monitoring Dashboard**
```python
from fastapi import FastAPI
from fastapi.responses import HTMLResponse

@app.get("/metrics", response_class=HTMLResponse)
async def metrics_dashboard():
    return f"""
    <html>
        <body>
            <h1>RAG System Metrics</h1>
            <p>Cache Hit Rate: {cache.hit_rate():.2%}</p>
            <p>Average Response Time: {cache.avg_response_time():.2f}ms</p>
            <p>Total Queries: {cache.total_queries}</p>
            <p>Memory Usage: {get_memory_usage():.1f}MB</p>
        </body>
    </html>
    """
```

### **ðŸš€ Deployment Checklist**
- [ ] Pre-embed all documents
- [ ] Warm up all models and caches
- [ ] Test with realistic query load
- [ ] Monitor memory usage patterns
- [ ] Set up error logging and monitoring
- [ ] Create backup API key strategies
- [ ] Test ngrok stability under load

This configuration should easily achieve your <300ms target while maintaining excellent accuracy. The local setup eliminates network latencies, and the aggressive caching will make repeat queries incredibly fast!

## 11. Hackathon API Specification

### **Required Endpoint Implementation**
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import httpx
import PyPDF2
import io
import asyncio

class HackathonRequest(BaseModel):
    documents: str  # PDF URL
    questions: List[str]

class HackathonResponse(BaseModel):
    answers: List[str]

@app.post("/hackrx/run", response_model=HackathonResponse)
async def hackrx_endpoint(
    request: HackathonRequest,
    authorization: str = Header(..., description="Bearer token")
):
    """Main hackathon endpoint - processes PDF and answers questions"""
    
    # Validate authorization
    expected_token = "Bearer 09988a27dc0bf3ef755e893e2e6650693e4009189215f7824b023cc07db59b1b"
    if authorization != expected_token:
        raise HTTPException(status_code=401, detail="Invalid authorization token")
    
    try:
        start_time = time.time()
        
        # Step 1: Download and process PDF
        pdf_content = await download_and_extract_pdf(request.documents)
        
        # Step 2: Process documents and build vector store
        await process_and_index_documents(pdf_content)
        
        # Step 3: Answer all questions in parallel
        answers = await answer_questions_batch(request.questions)
        
        total_time = (time.time() - start_time) * 1000
        print(f"âœ… Processed {len(request.questions)} questions in {total_time:.2f}ms")
        
        return HackathonResponse(answers=answers)
        
    except Exception as e:
        print(f"âŒ Error processing request: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Processing error: {str(e)}")

async def download_and_extract_pdf(pdf_url: str) -> str:
    """Download PDF from URL and extract text"""
    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.get(pdf_url)
        response.raise_for_status()
        
        # Extract text from PDF
        pdf_file = io.BytesIO(response.content)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        text_content = ""
        for page in pdf_reader.pages:
            text_content += page.extract_text() + "\n\n"
        
        return text_content

async def process_and_index_documents(content: str):
    """Process PDF content and build vector index"""
    # Use your optimized document processor
    processor = OptimizedDocumentProcessor()
    chunks = await processor.process_documents([content])
    
    # Get embeddings in batches
    texts = [chunk['content'] for chunk in chunks]
    embeddings = await get_embedding_batch(texts, batch_size=50)
    
    # Clear existing collection and add new documents
    collection.delete()  # Clear ChromaDB collection
    
    # Add to vector store with metadata
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        collection.add(
            embeddings=[embedding],
            documents=[chunk['content']],
            metadatas=[chunk['metadata']],
            ids=[f"doc_{i}"]
        )
    
    print(f"âœ… Indexed {len(chunks)} document chunks")

async def answer_questions_batch(questions: List[str]) -> List[str]:
    """Answer multiple questions efficiently"""
    # Process questions in parallel with limited concurrency
    semaphore = asyncio.Semaphore(5)  # Limit concurrent API calls
    
    async def answer_single_question(question: str) -> str:
        async with semaphore:
            return await answer_question_optimized(question)
    
    # Execute all questions in parallel
    tasks = [answer_single_question(q) for q in questions]
    answers = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Handle any exceptions
    final_answers = []
    for i, answer in enumerate(answers):
        if isinstance(answer, Exception):
            print(f"âŒ Error answering question {i}: {str(answer)}")
            final_answers.append("Unable to answer this question due to processing error.")
        else:
            final_answers.append(answer)
    
    return final_answers

async def answer_question_optimized(question: str) -> str:
    """Optimized single question answering"""
    try:
        # Step 1: Get question embedding
        question_embedding = await get_embedding_batch([question])
        
        # Step 2: Search vector store
        results = collection.query(
            query_embeddings=question_embedding,
            n_results=5,
            include=['documents', 'metadatas', 'distances']
        )
        
        # Step 3: Prepare context from top results
        context_parts = []
        for doc, metadata, distance in zip(
            results['documents'][0], 
            results['metadatas'][0], 
            results['distances'][0]
        ):
            if distance < 0.8:  # Only include relevant results
                context_parts.append(doc)
        
        context = "\n\n".join(context_parts[:3])  # Top 3 most relevant
        
        # Step 4: Generate answer with optimized prompt
        answer = await generate_answer_with_context(question, context)
        
        return answer.strip()
        
    except Exception as e:
        print(f"âŒ Error in answer_question_optimized: {str(e)}")
        return "Unable to find relevant information to answer this question."

async def generate_answer_with_context(question: str, context: str) -> str:
    """Generate answer using Gemini with optimized prompt"""
    
    # Optimized prompt for insurance policy Q&A
    prompt = f"""You are an expert insurance policy analyst. Based on the provided policy document context, answer the question accurately and concisely.

Context from Policy Document:
{context}

Question: {question}

Instructions:
- Provide a direct, factual answer based only on the policy document
- Include specific details like time periods, percentages, amounts when mentioned
- If the information is not in the context, state "Information not available in the provided document"
- Keep the answer concise but complete
- Use the exact terminology from the policy document

Answer:"""

    try:
        # Use Gemini 2.0 Flash Lite for fast generation
        response = await model.generate_content_async(
            prompt,
            generation_config={
                "temperature": 0.1,
                "top_p": 0.8,
                "max_output_tokens": 150,
            }
        )
        
        return response.text
        
    except Exception as e:
        print(f"âŒ Gemini API error: {str(e)}")
        
        # Fallback to Together.ai
        try:
            fallback_response = generate_with_together(question, context)
            return fallback_response
        except Exception as fallback_error:
            print(f"âŒ Fallback API error: {str(fallback_error)}")
            return "Unable to generate answer due to API issues."
```

### **Expected Request Format**
```json
{
    "documents": "https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D",
    "questions": [
        "What is the grace period for premium payment under the National Parivar Mediclaim Plus Policy?",
        "What is the waiting period for pre-existing diseases (PED) to be covered?",
        "Does this policy cover maternity expenses, and what are the conditions?",
        "What is the waiting period for cataract surgery?",
        "Are the medical expenses for an organ donor covered under this policy?",
        "What is the No Claim Discount (NCD) offered in this policy?",
        "Is there a benefit for preventive health check-ups?",
        "How does the policy define a 'Hospital'?",
        "What is the extent of coverage for AYUSH treatments?",
        "Are there sub-limits on room rent and ICU charges for Plan A?"
    ]
}
```

### **Expected Response Format**
```json
{
    "answers": [
        "A grace period of thirty days is provided for premium payment after the due date to renew or continue the policy without losing continuity benefits.",
        "There is a waiting period of thirty-six (36) months of continuous coverage from the first policy inception for pre-existing diseases and their direct complications to be covered.",
        "Yes, the policy covers maternity expenses, including childbirth and lawful medical termination of pregnancy. To be eligible, the female insured person must have been continuously covered for at least 24 months. The benefit is limited to two deliveries or terminations during the policy period.",
        "The policy has a specific waiting period of two (2) years for cataract surgery.",
        "Yes, the policy indemnifies the medical expenses for the organ donor's hospitalization for the purpose of harvesting the organ, provided the organ is for an insured person and the donation complies with the Transplantation of Human Organs Act, 1994.",
        "A No Claim Discount of 5% on the base premium is offered on renewal for a one-year policy term if no claims were made in the preceding year. The maximum aggregate NCD is capped at 5% of the total base premium.",
        "Yes, the policy reimburses expenses for health check-ups at the end of every block of two continuous policy years, provided the policy has been renewed without a break. The amount is subject to the limits specified in the Table of Benefits.",
        "A hospital is defined as an institution with at least 10 inpatient beds (in towns with a population below ten lakhs) or 15 beds (in all other places), with qualified nursing staff and medical practitioners available 24/7, a fully equipped operation theatre, and which maintains daily records of patients.",
        "The policy covers medical expenses for inpatient treatment under Ayurveda, Yoga, Naturopathy, Unani, Siddha, and Homeopathy systems up to the Sum Insured limit, provided the treatment is taken in an AYUSH Hospital.",
        "Yes, for Plan A, the daily room rent is capped at 1% of the Sum Insured, and ICU charges are capped at 2% of the Sum Insured. These limits do not apply if the treatment is for a listed procedure in a Preferred Provider Network (PPN)."
    ]
}
```

### **Additional Requirements & Dependencies**
```python
# Additional imports needed for hackathon endpoint
from fastapi import Header
import httpx
import PyPDF2
import io

# Install these additional packages
# pip install httpx PyPDF2 python-multipart
```

### **Performance Optimization for Hackathon Endpoint**

#### **A. PDF Processing Optimization**
```python
# Faster PDF processing with fallback
async def extract_pdf_text_optimized(pdf_url: str) -> str:
    """Optimized PDF text extraction with multiple strategies"""
    
    async with httpx.AsyncClient(timeout=30.0) as client:
        response = await client.get(pdf_url)
        response.raise_for_status()
        
        pdf_content = response.content
        
        # Try multiple extraction methods for best results
        try:
            # Method 1: PyPDF2 (fastest)
            return extract_with_pypdf2(pdf_content)
        except Exception as e1:
            try:
                # Method 2: pdfplumber (more accurate)
                import pdfplumber
                return extract_with_pdfplumber(pdf_content)
            except Exception as e2:
                # Method 3: pymupdf (fallback)
                import fitz
                return extract_with_pymupdf(pdf_content)

def extract_with_pypdf2(pdf_content: bytes) -> str:
    pdf_file = io.BytesIO(pdf_content)
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    
    text_content = ""
    for page in pdf_reader.pages:
        text_content += page.extract_text() + "\n\n"
    
    return text_content
```

#### **B. Batch Processing Strategy**
```python
# Process everything in optimal batches
EMBEDDING_BATCH_SIZE = 50  # Google's recommended batch size
QUESTION_CONCURRENCY = 5   # Parallel question processing
MAX_CONTEXT_TOKENS = 2000  # Keep context focused

async def process_hackathon_request_optimized(request: HackathonRequest):
    """Ultra-optimized request processing"""
    
    # Parallel PDF download and warm-up
    pdf_task = download_and_extract_pdf(request.documents)
    warmup_task = warm_up_models()  # Warm up while downloading
    
    pdf_content, _ = await asyncio.gather(pdf_task, warmup_task)
    
    # Fast document processing and indexing
    await process_and_index_documents_fast(pdf_content)
    
    # Batch answer questions with optimal concurrency
    answers = await answer_questions_batch_optimized(request.questions)
    
    return HackathonResponse(answers=answers)
```

### **ðŸŽ¯ Hackathon-Specific Recommendations**

1. **Pre-warm Everything**: Start your system 5 minutes before evaluation
2. **Error Handling**: Gracefully handle PDF parsing errors
3. **Timeout Management**: Set appropriate timeouts for all operations
4. **Logging**: Add comprehensive logging for debugging during evaluation
5. **Memory Management**: Monitor RAM usage with 10 questions processing
6. **API Rate Limits**: Implement backoff strategies for Google APIs

### **ðŸ“Š Expected Performance for Hackathon Endpoint**

| Operation | Target Time | Optimization Strategy |
|-----------|-------------|----------------------|
| PDF Download | 2-5 seconds | Parallel with warmup |
| PDF Text Extraction | 1-3 seconds | Multiple extraction methods |
| Document Chunking | 0.5-1 second | Optimized splitter |
| Embedding Generation | 3-8 seconds | Batch processing |
| Vector Indexing | 0.5-1 second | ChromaDB in-memory |
| 10 Questions Answering | 8-15 seconds | Parallel processing |
| **Total Pipeline** | **15-33 seconds** | **Well within limits** |

### **ðŸš¨ Critical Success Tips**
- **Test with the exact PDF URL** provided in the spec
- **Validate response format** matches exactly
- **Handle edge cases** like missing information gracefully  
- **Monitor token usage** to avoid API limits
- **Keep answers concise** but complete (like the examples)
- **Test authorization header** handling